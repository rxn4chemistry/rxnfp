{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapters for SmilesTransformer models\n",
    "\n",
    "> Adapting SmilesTransformer models to use a SmilesTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This extension has only been tested with simpletransformers==0.34.4\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "# optional\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import logging\n",
    "import random\n",
    "import warnings\n",
    "import pkg_resources\n",
    "import sklearn\n",
    "\n",
    "from transformers import (\n",
    "    BertConfig, BertForMaskedLM, AlbertConfig, AlbertForMaskedLM\n",
    ")\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "    wandb_available = True\n",
    "except ImportError:\n",
    "    wandb_available = False\n",
    "\n",
    "from rxnfp.tokenization import SmilesTokenizer\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "try: \n",
    "    import simpletransformers\n",
    "    logger.warning(\"This extension has only been tested with simpletransformers==0.34.4\")\n",
    "except ImportError:\n",
    "    raise ImportError('To use this extension, please install simpletransformers (tested with . \"pip install simpletransformers==0.34.4\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SmilesLanguageModelingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# optional\n",
    "from simpletransformers.config.global_args import global_args\n",
    "from simpletransformers.language_modeling import (\n",
    "    LanguageModelingModel\n",
    "\n",
    ")    \n",
    "\n",
    "class SmilesLanguageModelingModel(LanguageModelingModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type,\n",
    "        model_name,\n",
    "        generator_name=None,\n",
    "        discriminator_name=None,\n",
    "        train_files=None,\n",
    "        args=None,\n",
    "        use_cuda=True,\n",
    "        cuda_device=-1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        Initializes a LanguageModelingModel.\n",
    "        Main difference to https://github.com/ThilinaRajapakse/simpletransformers/blob/master/simpletransformers/classification/classification_model.py\n",
    "        is that it uses a SmilesTokenizer instead of the original Tokenizer.\n",
    "        Args:\n",
    "            model_type: The type of model bert (other model types could be implemented)\n",
    "            model_name: Default Transformer model name or path to a directory containing Transformer model file (pytorch_nodel.bin).\n",
    "            generator_name (optional): A pretrained model name or path to a directory containing an ELECTRA generator model.\n",
    "            discriminator_name (optional): A pretrained model name or path to a directory containing an ELECTRA discriminator model.\n",
    "            args (optional): Default args will be used if this parameter is not provided. If provided, it should be a dict containing the args that should be changed in the default args.\n",
    "            train_files (optional): List of files to be used when training the tokenizer.\n",
    "            use_cuda (optional): Use GPU if available. Setting to False will force model to use CPU only.\n",
    "            cuda_device (optional): Specific GPU that should be used. Will use the first available GPU by default.\n",
    "            **kwargs (optional): For providing proxies, force_download, resume_download, cache_dir and other options specific to the 'from_pretrained' implementation where this will be supplied.\n",
    "        \"\"\"  # noqa: ignore flake8\"\n",
    "        \n",
    "        MODEL_CLASSES = {\n",
    "            \"bert\": (BertConfig, BertForMaskedLM, SmilesTokenizer),\n",
    "            \"albert\": (AlbertConfig, AlbertForMaskedLM, SmilesTokenizer)\n",
    "\n",
    "\n",
    "        }\n",
    "        \n",
    "        if model_type not in MODEL_CLASSES.keys():\n",
    "            raise NotImplementedException(f\"Currently the following model types are implemented: {MODEL_CLASSES.keys()}\")\n",
    "\n",
    "        if args and \"manual_seed\" in args:\n",
    "            random.seed(args[\"manual_seed\"])\n",
    "            np.random.seed(args[\"manual_seed\"])\n",
    "            torch.manual_seed(args[\"manual_seed\"])\n",
    "            if \"n_gpu\" in args and args[\"n_gpu\"] > 0:\n",
    "                torch.cuda.manual_seed_all(args[\"manual_seed\"])\n",
    "\n",
    "        self.args = {\n",
    "            \"block_size\": -1,\n",
    "            \"config_name\": None,\n",
    "            \"dataset_class\": None,\n",
    "            \"dataset_type\": \"None\",\n",
    "            \"discriminator_config\": {},\n",
    "            \"discriminator_loss_weight\": 50,\n",
    "            \"generator_config\": {},\n",
    "            \"max_steps\": -1,\n",
    "            \"min_frequency\": 2,\n",
    "            \"mlm\": True,\n",
    "            \"mlm_probability\": 0.15,\n",
    "            \"sliding_window\": False,\n",
    "            \"special_tokens\": [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    "            \"stride\": 0.8,\n",
    "            \"tie_generator_and_discriminator_embeddings\": True,\n",
    "            \"tokenizer_name\": None,\n",
    "            \"vocab_size\": None,\n",
    "            \"local_rank\": -1,\n",
    "        }\n",
    "        \n",
    "    \n",
    "        self.args.update(global_args)\n",
    "\n",
    "        saved_model_args = self._load_model_args(model_name)\n",
    "        if saved_model_args:\n",
    "            self.args.update(saved_model_args)\n",
    "\n",
    "        if args:\n",
    "            self.args.update(args)\n",
    "\n",
    "        if self.args[\"local_rank\"] != -1:\n",
    "            logger.info(f'local_rank: {self.args[\"local_rank\"]}')\n",
    "            torch.distributed.init_process_group(backend=\"nccl\")\n",
    "            cuda_device = self.args[\"local_rank\"]\n",
    "\n",
    "        if use_cuda:\n",
    "            if torch.cuda.is_available():\n",
    "                if cuda_device == -1:\n",
    "                    self.device = torch.device(\"cuda\")\n",
    "                else:\n",
    "                    self.device = torch.device(f\"cuda:{cuda_device}\")\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"'use_cuda' set to True when cuda is unavailable.\"\n",
    "                    \" Make sure CUDA is available or set use_cuda=False.\"\n",
    "                )\n",
    "        else:\n",
    "            self.device = \"cpu\"\n",
    "\n",
    "        self.results = {}\n",
    "\n",
    "        if not use_cuda:\n",
    "            self.args[\"fp16\"] = False\n",
    "\n",
    "        self.args[\"model_name\"] = model_name\n",
    "        self.args[\"model_type\"] = model_type\n",
    "\n",
    "        config_class, model_class, tokenizer_class = MODEL_CLASSES[model_type]\n",
    "        self.tokenizer = tokenizer_class(self.args[\"vocab_path\"])\n",
    "                        \n",
    "        self.config = config_class(**self.args[\"config\"], **kwargs)\n",
    "            \n",
    "        self.config.vocab_size = len(self.tokenizer)\n",
    "\n",
    "        \n",
    "        if self.args[\"block_size\"] <= 0:\n",
    "            self.args[\"block_size\"] = min(self.args[\"max_seq_length\"], self.tokenizer.max_len)\n",
    "        else:\n",
    "            self.args[\"block_size\"] = min(self.args[\"block_size\"], self.tokenizer.max_len, self.args[\"max_seq_length\"])\n",
    "\n",
    "        if self.args[\"model_name\"]:\n",
    "            self.model = model_class.from_pretrained(\n",
    "                model_name, config=self.config, cache_dir=self.args[\"cache_dir\"], **kwargs,\n",
    "            )\n",
    "        else:\n",
    "            logger.info(\" Training language model from scratch\")\n",
    "            \n",
    "            self.model = model_class(config=self.config)\n",
    "            model_to_resize = self.model.module if hasattr(self.model, \"module\") else self.model\n",
    "            model_to_resize.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "        if model_type in [\"camembert\", \"xlmroberta\"]:\n",
    "            warnings.warn(\n",
    "                f\"use_multiprocessing automatically disabled as {model_type}\"\n",
    "                \" fails when using multiprocessing for feature conversion.\"\n",
    "            )\n",
    "            self.args[\"use_multiprocessing\"] = False\n",
    "\n",
    "        if self.args[\"wandb_project\"] and not wandb_available:\n",
    "            warnings.warn(\"wandb_project specified but wandb is not available. Wandb disabled.\")\n",
    "            self.args[\"wandb_project\"] = None\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting 'max_len_single_sentence' is now deprecated. This value is automatically set up.\n",
      "Setting 'max_len_sentences_pair' is now deprecated. This value is automatically set up.\n"
     ]
    }
   ],
   "source": [
    "# optional\n",
    "model = SmilesLanguageModelingModel(model_type='bert', model_name=None, args={'vocab_path': '../data/uspto_1000/vocab.txt'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  SmilesClassificationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# optional\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "from simpletransformers.classification.transformer_models.bert_model import BertForSequenceClassification\n",
    "\n",
    "\n",
    "class SmilesClassificationModel(ClassificationModel):\n",
    "    def __init__(\n",
    "        self, model_type, model_name, num_labels=None, weight=None, freeze_encoder=False, freeze_all_but_one=False, args=None, use_cuda=True, cuda_device=-1, **kwargs,\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        Initializes a SmilesClassificationModel model.\n",
    "        \n",
    "        Main difference to https://github.com/ThilinaRajapakse/simpletransformers/blob/master/simpletransformers/classification/classification_model.py\n",
    "        is that it uses a SmilesTokenizer instead of the original Tokenizer\n",
    "\n",
    "        Args:\n",
    "            model_type: The type of model (bert, xlnet, xlm, roberta, distilbert)\n",
    "            model_name: The exact architecture and trained weights to use. This may be a Hugging Face Transformers compatible pre-trained model, a community model, or the path to a directory containing model files.\n",
    "            num_labels (optional): The number of labels or classes in the dataset.\n",
    "            weight (optional): A list of length num_labels containing the weights to assign to each label for loss calculation.\n",
    "            args (optional): Default args will be used if this parameter is not provided. If provided, it should be a dict containing the args that should be changed in the default args.\n",
    "            use_cuda (optional): Use GPU if available. Setting to False will force model to use CPU only.\n",
    "            cuda_device (optional): Specific GPU that should be used. Will use the first available GPU by default.\n",
    "            **kwargs (optional): For providing proxies, force_download, resume_download, cache_dir and other options specific to the 'from_pretrained' implementation where this will be supplied.\n",
    "        \"\"\"  # noqa: ignore flake8\"\n",
    "\n",
    "        MODEL_CLASSES = {\n",
    "            \"bert\": (BertConfig, BertForSequenceClassification, SmilesTokenizer),\n",
    "        }\n",
    "        \n",
    "        if model_type not in MODEL_CLASSES.keys():\n",
    "            raise NotImplementedException(f\"Currently the following model types are implemented: {MODEL_CLASSES.keys()}\")\n",
    "\n",
    "        if args and \"manual_seed\" in args:\n",
    "            random.seed(args[\"manual_seed\"])\n",
    "            np.random.seed(args[\"manual_seed\"])\n",
    "            torch.manual_seed(args[\"manual_seed\"])\n",
    "            if \"n_gpu\" in args and args[\"n_gpu\"] > 0:\n",
    "                torch.cuda.manual_seed_all(args[\"manual_seed\"])\n",
    "\n",
    "        self.args = {\n",
    "            \"sliding_window\": False,\n",
    "            \"tie_value\": 1,\n",
    "            \"stride\": 0.8,\n",
    "            \"regression\": False,\n",
    "            \"lazy_text_column\": 0,\n",
    "            \"lazy_text_a_column\": None,\n",
    "            \"lazy_text_b_column\": None,\n",
    "            \"lazy_labels_column\": 1,\n",
    "            \"lazy_header_row\": True,\n",
    "            \"lazy_delimiter\": \"\\t\",\n",
    "        }\n",
    "\n",
    "        self.args.update(global_args)\n",
    "\n",
    "        saved_model_args = self._load_model_args(model_name)\n",
    "        if saved_model_args:\n",
    "            self.args.update(saved_model_args)\n",
    "\n",
    "        if args:\n",
    "            self.args.update(args)\n",
    "\n",
    "        config_class, model_class, tokenizer_class = MODEL_CLASSES[model_type]\n",
    "        if num_labels:\n",
    "            self.config = config_class.from_pretrained(model_name, num_labels=num_labels, **self.args[\"config\"])\n",
    "            self.num_labels = num_labels\n",
    "        else:\n",
    "            self.config = config_class.from_pretrained(model_name, **self.args[\"config\"])\n",
    "            self.num_labels = self.config.num_labels\n",
    "        self.weight = weight\n",
    "\n",
    "        if use_cuda:\n",
    "            if torch.cuda.is_available():\n",
    "                if cuda_device == -1:\n",
    "                    self.device = torch.device(\"cuda\")\n",
    "                else:\n",
    "                    self.device = torch.device(f\"cuda:{cuda_device}\")\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"'use_cuda' set to True when cuda is unavailable.\"\n",
    "                    \" Make sure CUDA is available or set use_cuda=False.\"\n",
    "                )\n",
    "        else:\n",
    "            self.device = \"cpu\"\n",
    "\n",
    "        if self.weight:\n",
    "            self.model = model_class.from_pretrained(\n",
    "                model_name, config=self.config, weight=torch.Tensor(self.weight).to(self.device), **kwargs,\n",
    "            )\n",
    "        else:\n",
    "            self.model = model_class.from_pretrained(model_name, config=self.config, **kwargs)\n",
    "\n",
    "        self.results = {}\n",
    "\n",
    "        if not use_cuda:\n",
    "            self.args[\"fp16\"] = False\n",
    "\n",
    "        self.tokenizer = tokenizer_class(os.path.join(model_name, 'vocab.txt'))\n",
    "        \n",
    "        if freeze_encoder:\n",
    "            for name, param in self.model.named_parameters():\n",
    "                if 'classifier' in name:\n",
    "                    continue\n",
    "                param.requires_grad = False\n",
    "        elif freeze_all_but_one:\n",
    "            n_layers = self.model.config.num_hidden_layers\n",
    "            for name, param in self.model.named_parameters():\n",
    "                if str(n_layers-1) in name:\n",
    "                    continue\n",
    "                elif 'classifier' in name:\n",
    "                    continue\n",
    "                elif 'pooler' in name:\n",
    "                    continue\n",
    "                param.requires_grad = False\n",
    "            \n",
    "\n",
    "        self.args[\"model_name\"] = model_name\n",
    "        self.args[\"model_type\"] = model_type\n",
    "\n",
    "\n",
    "        if self.args[\"wandb_project\"] and not wandb_available:\n",
    "            warnings.warn(\"wandb_project specified but wandb is not available. Wandb disabled.\")\n",
    "            self.args[\"wandb_project\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
