{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a SMILES language model from scratch\n",
    "\n",
    "> Tutorial how to train a reaction language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n",
      "This extension has only been tested with simpletransformers==0.34.4\n"
     ]
    }
   ],
   "source": [
    "# optional\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import logging\n",
    "import random\n",
    "from rxnfp.models import SmilesLanguageModelingModel\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track the training \n",
    "\n",
    "Will be using wandb to keep track of our training. You can use the an account on [wandb](https://www.wandb.com) or create an own instance following the instruction in the [documentation](https://docs.wandb.com/self-hosted).\n",
    "\n",
    "If you then create an `.env` file in the root folder and specify the `WANDB_API_KEY=` (and the `WANDB_BASE_URL=`), you can use dotenv to load those enviroment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optional\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup MLM training \n",
    "\n",
    "Choose the hyperparameter you want and start the training. The default parameters will train a BERT model with 12 layers and 4 attention heads per layer. The training task is Masked Language Modeling (MLM), where tokens from the input reactions are randomly masked and predicted by the model given the context. \n",
    "\n",
    "After defining the config, the training is launched in 3 lines of code using our adapter written for the  [SimpleTransformers](https://simpletransformers.ai) library (based on huggingface [Transformers](https://github.com/huggingface/transformers).  \n",
    "\n",
    "To make it work you will have to install simpletransformers:\n",
    "\n",
    "```bash\n",
    "pip install simpletransformers==0.34.4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional\n",
    "config = {\n",
    "  \"architectures\": [\n",
    "    \"BertForMaskedLM\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.1,\n",
    "  \"hidden_size\": 256,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 512,\n",
    "  \"layer_norm_eps\": 1e-12,\n",
    "  \"max_position_embeddings\": 512,\n",
    "  \"model_type\": \"bert\",\n",
    "  \"num_attention_heads\": 4,\n",
    "  \"num_hidden_layers\": 12,\n",
    "  \"pad_token_id\": 0,\n",
    "\"type_vocab_size\": 2,\n",
    "}\n",
    "vocab_path = '../data/uspto_1k_TPL/individual_files/vocab.txt'\n",
    "\n",
    "args = {'config': config, \n",
    "        'vocab_path': vocab_path, \n",
    "        'wandb_project': 'uspto_mlm_temp_1000',\n",
    "        'train_batch_size': 32,\n",
    "        'manual_seed': 42,\n",
    "        \"fp16\": False,\n",
    "        \"num_train_epochs\": 50,\n",
    "        'max_seq_length': 256,\n",
    "        'evaluate_during_training': True,\n",
    "        'overwrite_output_dir': True,\n",
    "        'output_dir': '../out/bert_mlm_1k_tpl',\n",
    "        'learning_rate': 1e-4\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting 'max_len_single_sentence' is now deprecated. This value is automatically set up.\n",
      "Setting 'max_len_sentences_pair' is now deprecated. This value is automatically set up.\n"
     ]
    }
   ],
   "source": [
    "# optional\n",
    "model = SmilesLanguageModelingModel(model_type='bert', model_name=None, args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional\n",
    "train_file = '../data/uspto_1k_TPL/individual_files/mlm_train_file.txt'\n",
    "eval_file = '../data/uspto_1k_TPL/individual_files/mlm_eval_file_1k.txt'\n",
    "model.train_model(train_file=train_file, eval_file=eval_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
